{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUWZUoPnfsRky48lgnM5TG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meshal6299/arabic-image-captioning/blob/main/notebooks/02_BLIP_Arabic_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbffqrSFzTUB"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Libraries\n",
        "!pip install transformers torch datasets pillow\n",
        "!pip install evaluate nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dSYPqEobTSjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Import All Libraries\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm # For a nice progress bar\n",
        "\n",
        "# Evaluation libraries\n",
        "import evaluate\n",
        "import nltk\n",
        "nltk.download('punkt') # Download tokenizer for BLEU/ROUGE"
      ],
      "metadata": {
        "id": "_jAWWx6uTYrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define File Paths and Device\n",
        "\n",
        "# --- !! CHANGE THESE PATHS !! ---\n",
        "# This must be the same path as in your first notebook\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/PR Project/dataset\"\n",
        "# --- !! -------------------- !! ---\n",
        "\n",
        "# Path to the model you saved\n",
        "MODEL_PATH = os.path.join(PROJECT_PATH, \"arabic_blip_model\")\n",
        "\n",
        "# We also need the original dataset file and images to create our test set\n",
        "DATASET_FILE = os.path.join(PROJECT_PATH, \"Arabic_Description_sample.csv\")\n",
        "IMAGE_DIR = os.path.join(PROJECT_PATH, \"Images\")\n",
        "\n",
        "# Set up the device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "Kmwvo9jTTs2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Load Your Fine-Tuned Model\n",
        "print(\"Loading fine-tuned model and processor...\")\n",
        "processor = BlipProcessor.from_pretrained(MODEL_PATH)\n",
        "model = BlipForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
        "\n",
        "# Move the model to the GPU\n",
        "model.to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "id": "wAkn2TaxULLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Re-load the Dataset Class\n",
        "\n",
        "# This is the same class from the first notebook\n",
        "class ArabicImageCaptionDataset(Dataset):\n",
        "    def __init__(self, dataset_file, image_dir, processor, max_length=128):\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "        self.data = []\n",
        "\n",
        "        with open(dataset_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(',', 1)\n",
        "                if len(parts) == 2:\n",
        "                    image_name, text = parts\n",
        "                    self.data.append({\"image_name\": image_name, \"text\": text})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image_path = os.path.join(self.image_dir, item[\"image_name\"])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image file not found {image_path}.\")\n",
        "            # Just return the text and a None for the image\n",
        "            return {\"text\": item['text'], \"image\": None}\n",
        "\n",
        "        return {\"text\": item['text'], \"image\": image, \"image_name\": item['image_name']}"
      ],
      "metadata": {
        "id": "he1y1G31UQfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create the Validation Set\n",
        "\n",
        "# We are NOT processing the text here, just loading it\n",
        "# We pass the processor just in case, but we won't use it for text\n",
        "full_dataset = ArabicImageCaptionDataset(dataset_file=DATASET_FILE,\n",
        "                                         image_dir=IMAGE_DIR,\n",
        "                                         processor=processor)\n",
        "\n",
        "# IMPORTANT: Set a random seed\n",
        "# This ensures we get the EXACT same 90/10 split as in Notebook 1\n",
        "# Use the same number you used for the split in your first notebook\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Split into training and validation\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Dataset loaded. Using validation set of size: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "KPu_18uKUX29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Helper Function for Inference (Corrected)\n",
        "\n",
        "def generate_caption(image):\n",
        "    \"\"\"\n",
        "    Takes a PIL Image and generates a caption using the fine-tuned model.\n",
        "    \"\"\"\n",
        "    if image is None:\n",
        "        return \"Error: Image not found\"\n",
        "\n",
        "    # Process the image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate caption (unprompted)\n",
        "    # This matches how the model was trained\n",
        "    outputs = model.generate(pixel_values=inputs.pixel_values,\n",
        "                             max_length=128)\n",
        "\n",
        "    # Decode the generated text\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return caption.strip()"
      ],
      "metadata": {
        "id": "AYDnnoBXUeNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test on a Single Image (Corrected Display)\n",
        "\n",
        "from IPython.display import display, HTML # <-- Import HTML\n",
        "\n",
        "# Get the first item from our validation set\n",
        "item = val_dataset[0]\n",
        "image = item['image']\n",
        "reference_caption = item['text']\n",
        "\n",
        "# Generate a caption with our model\n",
        "generated_caption = generate_caption(image)\n",
        "\n",
        "# Show the image\n",
        "print(f\"Displaying image: {item['image_name']}\")\n",
        "display(image.resize((300, 300))) # Show a smaller version\n",
        "\n",
        "print(\"---\")\n",
        "# --- THIS IS THE FIX ---\n",
        "# Use HTML to display the text with Right-to-Left (RTL) direction\n",
        "display(HTML(f'<p style=\"direction: rtl;\"><b>Reference (Human):</b> {reference_caption}</p>'))\n",
        "display(HTML(f'<p style=\"direction: rtl;\"><b>Generated (Model):</b> {generated_caption}</p>'))"
      ],
      "metadata": {
        "id": "t-AENds9UwL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}